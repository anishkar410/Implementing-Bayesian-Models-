# -*- coding: utf-8 -*-
"""Linear regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q_if5Y6bGgrM8Mms1SVM_e-vpCY5uymy

**Assignment 4**
--
*Anishka Raj*

---

**Part 1: *A simple linear regression: Power posing and testosterone***
---
"""

import pandas as pd
import statsmodels.api as sm

# Load dataset from CSV file
df_powerpose = pd.read_csv('/content/df_powerpose_99f68d8a-2128-47c5-84c4-93af776c0468 (2).csv')

# Calculate change in testosterone levels after treatment
df_powerpose['delta_testosterone'] = df_powerpose['testm2'] - df_powerpose['testm1']

# Define independent (X) and dependent variables (Y)
# Independent variable: 'hptreat' mapped to numeric values ('Low' -> 0, 'High' -> 1)
independent_var = df_powerpose['hptreat'].map({'Low': 0, 'High': 1})
dependent_var = df_powerpose['delta_testosterone']

# Add constant to the independent variable for the intercept in the regression model
independent_var = sm.add_constant(independent_var)

# Fit the Ordinary Least Squares (OLS) regression model
model = sm.OLS(dependent_var, independent_var).fit()

# Print summary of the regression results
print("Summary of linear regression results:")
print(model.summary())

from google.colab import drive
drive.mount('/content/drive')

"""**Part 2:  *Poisson regression models and hypothesis testing***
--

---

**Exercise 2.1**
---

1. A function calculate_crossings is defined wherein lambda_i is calculated using the formula given. After that, crossings are generated as random values samples taken from the poisson distribution of lambda_i calculated above.
2. Usage of the above model demonstrated using example values of parameters sentence_length, alpha and beta. It prints the number of crossings.
"""

import numpy as np
def calculate_crossings(sentence_length, alpha, beta):
    # Calculate lambda_i using the given formula
    lambda_i = np.exp(alpha + (beta * sentence_length))

    # Generate the number of crossings using Poisson distribution
    crossings = np.random.poisson(lambda_i)

    return crossings

# Example usage
sentence_length = 11
alpha = 0.2
beta = 0.03

# Calculate the number of crossings
num_crossings = calculate_crossings(sentence_length, alpha, beta)

# Print the result
print("Number of crossings:", num_crossings)

"""**Exercise 2.2**
---

We use the given prior data:

**α ~ Normal(0.15,0.1)**

**β ~ Normal(0.25,0.05)**

and the sentence length = 4, as given.
Prior predictions are calculated using the same calculate_crossings function. The result of the function, ni is the number of crossings sampled from the poisson distribution of lambda_i. These results are stored in a predictions[ ] array, which gives the required prior predictions for given sentence length of 4.
"""

import numpy as np

def compute_crossings_rate(alpha_param, beta_param, sentence_length):
    # Calculate the number of crossings based on given alpha, beta parameters,
    # and sentence length using a Poisson distribution.
    # - alpha_param (float): Alpha parameter for the exponential calculation.
    # - beta_param (float): Beta parameter for the exponential calculation.
    # - sentence_length (int): Length of the sentence.
    # - crossings (int): Number of crossings computed.

    lambda_i = np.exp(alpha_param + beta_param * sentence_length)
    crossings = np.random.poisson(lambda_i)
    return crossings

def generate_prior_predictions(sentence_length, num_samples=10000):
    # Generate prior predictions based on sampled alpha and beta parameters,
    # using a normal distribution, for a given sentence length.
    # - sentence_length (int): Length of the sentence.
    # - num_samples (int): Number of samples to generate (default is 10,000).
    # - predictions (list): List of predictions (number of crossings) for each sample.
    # Generate samples from normal distribution for alpha and beta
    alpha_samples = np.random.normal(loc=0.15, scale=0.1, size=num_samples)
    beta_samples = np.random.normal(loc=0.25, scale=0.05, size=num_samples)

    predictions = []

    # Calculate crossings for each sampled alpha and beta
    for alpha, beta in zip(alpha_samples, beta_samples):
        crossings = compute_crossings_rate(alpha, beta, sentence_length)
        predictions.append(crossings)

    return predictions

# Example usage:
sentence_length = 4
prior_predictions = generate_prior_predictions(sentence_length)

# Print some prior predictions for inspection
print(f"Prior predictions of the model for sentence length {sentence_length}:")
print(prior_predictions[:20])  # Print first 20 predictions

"""**Exercise 2.3**
---

We defined separate variables for both models M1 and M2

X_M1: stores sentence length

Y_M1: stores number of crossings

X_M2: stores sentence length along with an id indicating language(0 for English, 1 for German)

Y_M2: stores number of crossings

Model M1:

We use sentence length data independent of language to see its effect on number of crossings.

Model M2:
We use sentence length data along with the language data to see how both of them interact, by observing their effect on number of crossings.
"""

import pandas as pd
import statsmodels.api as sm

# Load data from CSV file
crossings_data = pd.read_csv('/content/crossings_24a167f3-2f8f-4f5c-bca9-884567bb1c33.csv')

# Model M1: Poisson GLM with one predictor (s.length)
# Prepare predictor and response variables for Model M1
X_M1 = crossings_data[['s.length']]  # Predictor variable: s.length
X_M1 = sm.add_constant(X_M1)  # Adding intercept to predictor variables
y_M1 = crossings_data['nCross']  # Response variable: nCross
# Fit Poisson GLM for Model M1
model_M1 = sm.GLM(y_M1, X_M1, family=sm.families.Poisson()).fit()
summary_M1 = model_M1.summary()  # Generate summary of Model M1

# Print summary of Model M1
print("Summary of Model M1:\n", summary_M1)

# Model M2: Poisson GLM with interaction between s.length and s.id
# Create binary indicator (s.id) for 'German' language
crossings_data['s.id'] = (crossings_data['Language'] == 'German').astype(int)
# Prepare predictor variables including interaction term for Model M2
X_M2 = crossings_data[['s.length', 's.id']]  # Predictors: s.length, s.id (binary indicator)
X_M2['interaction'] = X_M2['s.length'] * X_M2['s.id']  # Interaction term: s.length * s.id
X_M2 = sm.add_constant(X_M2)  # Adding intercept to predictor variables
y_M2 = crossings_data['nCross']  # Response variable: nCross
# Fit Poisson GLM for Model M2
model_M2 = sm.GLM(y_M2, X_M2, family=sm.families.Poisson()).fit()
summary_M2 = model_M2.summary()  # Generate summary of Model M2

# Print summary of Model M2
print("Summary of Model M2:\n", summary_M2)

"""Effect of Sentence Length: Both models show a positive and significant effect of sentence length on the number of crossings (nCross).

Language Difference: Model M2 explicitly captures differences between English and German sentences (s.id), showing that German sentences tend to have fewer crossings than English sentences.

Interaction Effect: Model M2 includes an interaction term (interaction), indicating that the effect of sentence length on crossings varies depending on the language.

**Exercise 2.4**
---
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt

# Load data from CSV file
crossings_data = pd.read_csv('/content/crossings_24a167f3-2f8f-4f5c-bca9-884567bb1c33.csv')

# Plot average rate of crossings by sentence length
crossings_data.groupby('s.length')['nCross'].mean().plot(kind='bar')
plt.xlabel('Sentence Length')
plt.ylabel('Average Rate of Crossings')
plt.title('Average Rate of Crossings by Sentence Length')
plt.show()

# Centering variables for Model M2
crossings_data['sentence_length_centered'] = crossings_data['s.length'] - crossings_data['s.length'].mean()
crossings_data['language_ind_centered'] = crossings_data['s.id'] - crossings_data['s.id'].mean()

# K-fold cross-validation setup
k = 10  # Number of folds for cross-validation
kf = KFold(n_splits=k, shuffle=True, random_state=1)  # Initialize KFold with shuffle and random state

# Function to calculate log predictive density (LPD)
def calculate_lpd(model, X_test, y_test):
# Calculate log predictive density (LPD) for a given model and test data.
# - model: Fitted statistical model (statsmodels GLM object)
# - X_test: Predictors (numpy array or pandas DataFrame) for test data
# - y_test: Response variable (numpy array or pandas Series) for test data
# - lpd: Log predictive density (float)
    predictions = model.get_prediction(X_test)  # Get predictions for test data
    mean_pred = predictions.predicted_mean  # Mean predictions
    lpd = np.sum(y_test * np.log(mean_pred) - mean_pred)  # Calculate Poisson log likelihood
    return lpd

# Arrays to store log predictive densities (LPD) for each fold
lpd_M1 = np.zeros(k)
lpd_M2 = np.zeros(k)

# K-fold cross-validation loop
for i, (train_index, test_index) in enumerate(kf.split(crossings_data)):
    # Split data into training and test sets for current fold
    train_data, test_data = crossings_data.iloc[train_index], crossings_data.iloc[test_index]

    # Model M1: Poisson GLM with centered sentence length
    X_train_M1 = train_data[['sentence_length_centered']]
    X_train_M1 = sm.add_constant(X_train_M1)  # Add intercept
    y_train_M1 = train_data['nCross']
    X_test_M1 = test_data[['sentence_length_centered']]
    X_test_M1 = sm.add_constant(X_test_M1)  # Add intercept
    y_test_M1 = test_data['nCross']

    # Fit Poisson GLM for Model M1
    model_M1 = sm.GLM(y_train_M1, X_train_M1, family=sm.families.Poisson()).fit()
    # Calculate LPD for Model M1 on test data
    lpd_M1[i] = calculate_lpd(model_M1, X_test_M1, y_test_M1)

    # Model M2: Poisson GLM with centered variables and interaction
    X_train_M2 = train_data[['sentence_length_centered', 'language_ind_centered']].copy()
    X_train_M2['interaction'] = X_train_M2['sentence_length_centered'] * X_train_M2['language_ind_centered']
    X_train_M2 = sm.add_constant(X_train_M2)  # Add intercept
    y_train_M2 = train_data['nCross']
    X_test_M2 = test_data[['sentence_length_centered', 'language_ind_centered']].copy()
    X_test_M2['interaction'] = X_test_M2['sentence_length_centered'] * X_test_M2['language_ind_centered']
    X_test_M2 = sm.add_constant(X_test_M2)  # Add intercept
    y_test_M2 = test_data['nCross']

    # Fit Poisson GLM for Model M2
    model_M2 = sm.GLM(y_train_M2, X_train_M2, family=sm.families.Poisson()).fit()
    # Calculate LPD for Model M2 on test data
    lpd_M2[i] = calculate_lpd(model_M2, X_test_M2, y_test_M2)

# Calculate mean log predictive densities for Model M1 and Model M2
mean_lpd_M1 = np.mean(lpd_M1)
mean_lpd_M2 = np.mean(lpd_M2)

# Print results
print(f'Mean log predictive density for Model M1: {mean_lpd_M1}')
print(f'Mean log predictive density for Model M2: {mean_lpd_M2}')

# Calculate evidence in favor of Model M2 over Model M1
evidence = mean_lpd_M2 - mean_lpd_M1
print(f'Evidence in favor of Model M2 over Model M1: {evidence}')

"""*Visualizing Data*: First, we visualize the average rate of crossings by sentence length using a bar graph to gain an initial understanding of the data distribution.

*Centering Predictors* : To standardize our predictors, we center s.length (sentence length) and s.id (language id) around zero. This preprocessing step ensures that both predictors have a mean of zero.

*k-fold Cross-Validation Setup* : We implement a 10-fold cross-validation (k=10) using KFold from sklearn.model_selection. This technique splits the data into 10 folds, enabling robust evaluation of our models.

*Model Fitting and Evaluation* : Within each fold of the cross-validation:

- We fit Model M1 and Model M2 using sm.GLM from statsmodels.api, specifying the Poisson family for likelihood estimation.
- For each fold, a helper function calculate_lpd computes the Poisson log likelihood based on predicted means (get_prediction) derived from the fitted models.

*Comparing Models* : Post cross-validation, we compute the mean log predictive density (LPD) for both Model M1 and Model M2 across all folds to assess their predictive performance.

*Evidence Calculation*: Finally, we quantify the evidence in favor of Model M2 over Model M1 by subtracting their respective mean log predictive densities. This metric helps determine which model provides a better fit to the data based on the Poisson log likelihood.
"""