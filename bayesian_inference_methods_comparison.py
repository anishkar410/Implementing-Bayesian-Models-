# -*- coding: utf-8 -*-
"""Bayesian_inference_methods_comparison

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZLTlLudM8-qMi2nX_m_RkNRdbYI1p1SB
"""

# Part 1: Estimating the posterior distribution using different computational methods
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta, binom

# 1. Analytically-derived Posterior Distribution of θ
a, b = 135, 67  # Parameters for the Beta distribution
theta = np.linspace(0, 1, 1000)  # Range of θ values
posterior = beta.pdf(theta, a, b)  # Compute posterior density
plt.plot(theta, posterior, label='Beta(135, 67)')
plt.xlabel(r'$\theta$')
plt.ylabel('Density')
plt.title('Analytically-derived Posterior Distribution of $\\theta$')
plt.legend()
plt.grid(True)
plt.show()

# 2. Posterior Density using Grid Approximation
y = np.array([10, 15, 15, 14, 14, 14, 13, 11, 12, 16])  # Observed data
n = 20  # Number of trials

theta_grid = np.linspace(0, 1, 1000)  # Grid of θ values
df_posterior = pd.DataFrame(columns=["theta", "likelihood", "prior", "posterior"])
df_posterior["theta"] = theta_grid

# Compute likelihood and prior for each θ
likelihoods = []
priors = []
for theta in theta_grid:
    likelihood = np.prod(binom.pmf(y, n, theta))  # Binomial likelihood
    prior = beta.pdf(theta, 1, 1)  # Uniform prior (Beta(1,1))
    likelihoods.append(likelihood)
    priors.append(prior)

df_posterior["likelihood"] = likelihoods
df_posterior["prior"] = priors
marginal_likelihood = np.sum(df_posterior["likelihood"] * df_posterior["prior"])
df_posterior["posterior"] = df_posterior["likelihood"] * df_posterior["prior"] / marginal_likelihood

plt.plot(df_posterior["theta"], df_posterior["posterior"], label='Posterior Density')
plt.title("Posterior Density of θ using Grid Approximation")
plt.xlabel("θ")
plt.ylabel("Posterior Density")
plt.legend()
plt.grid(True)
plt.show()

# 3. Marginal Likelihood Estimation via Monte Carlo Integration
data = np.array([10, 15, 15, 14, 14, 14, 13, 11, 12, 16])
n_trials = 20
alpha_prior, beta_prior = 1, 1
N = 100000  # Number of samples

# Sample from the prior distribution
theta_samples = np.random.beta(alpha_prior, beta_prior, size=N)

# Compute the likelihood for each sample
likelihoods = np.prod(np.power(theta_samples[:, np.newaxis], data.sum()) *
                      np.power(1 - theta_samples[:, np.newaxis], n_trials * len(data) - data.sum()), axis=1)

# Estimate the marginal likelihood
marginal_likelihood = np.mean(likelihoods)
print(f"Estimated Marginal Likelihood: {marginal_likelihood}")

# 4. Importance Sampling for Posterior Distribution
N = 100000  # Number of samples

# Proposal distribution q(theta)
q_theta = np.random.beta(1 + np.sum(data), 1 + 10 * len(data), size=N)

# Sample from proposal distribution
theta_proposal = np.random.beta(2, 2, size=N)
likelihood_q = np.prod(np.power(theta_proposal[:, np.newaxis], data.sum()) *
                       np.power(1 - theta_proposal[:, np.newaxis], n_trials * len(data) - data.sum()), axis=1)

# Compute weights for importance sampling
weights = beta.pdf(q_theta, 1 + np.sum(data), 1 + 10 * len(data)) / beta.pdf(q_theta, 1, 1)

# Resample using weights
posterior_samples_importance = np.random.choice(q_theta, size=N//4, replace=True, p=weights/np.sum(weights))
print("Selected samples from the posterior distribution:")
print(posterior_samples_importance)

# 5. Metropolis-Hastings MCMC for Posterior Distribution
def log_posterior(theta, data, n, alpha_prior, beta_prior):
    if theta < 0 or theta > 1:
        return -np.inf
    else:
        likelihood = np.prod(theta**data.sum() * (1 - theta)**(n * len(data) - data.sum()))
        prior = theta**(alpha_prior - 1) * (1 - theta)**(beta_prior - 1)
        return np.log(likelihood * prior)

def metropolis_hastings(log_posterior, theta0, n_samples, proposal_std=0.1):
    samples = [theta0]
    current_theta = theta0
    for _ in range(n_samples):
        proposed_theta = np.random.normal(current_theta, proposal_std)
        log_alpha = log_posterior(proposed_theta, data, n_trials, alpha_prior, beta_prior) - \
                    log_posterior(current_theta, data, n_trials, alpha_prior, beta_prior)
        if np.log(np.random.uniform(0, 1)) < log_alpha:
            current_theta = proposed_theta
        samples.append(current_theta)
    return np.array(samples)

np.random.seed(123)
theta_init = 0.5
n_samples = 10000
samples = metropolis_hastings(log_posterior, theta_init, n_samples)

plt.figure(figsize=(10, 6))
plt.hist(samples, bins=60, density=True, color='lightgreen', edgecolor='black', alpha=0.7)
plt.title('Posterior Distribution of θ (MCMC)', fontsize=16)
plt.xlabel('θ')
plt.ylabel('Density')
plt.grid(True)
plt.show()

# 6. Comparing Analytical Posterior, Importance Sampling, and MCMC
# Define the analytical posterior distribution
alpha = 135
beta_param = 67
theta_values = np.linspace(0, 1, 1000)
posterior_dens = beta.pdf(theta_values, alpha, beta_param)

# Importance sampling
N_importance = 100000
q_theta_importance = np.random.beta(1 + np.sum(data), 1 + 10 * len(data), size=N_importance)
theta_proposal_importance = np.random.beta(2, 2, size=N_importance)
likelihood_q_importance = np.prod(np.power(theta_proposal_importance[:, np.newaxis], data.sum()) *
                                  np.power(1 - theta_proposal_importance[:, np.newaxis], n_trials * len(data) - data.sum()), axis=1)
weights_importance = beta.pdf(q_theta_importance, 1 + np.sum(data), 1 + 10 * len(data)) / beta.pdf(q_theta_importance, 1, 1)
posterior_samples_importance = np.random.choice(q_theta_importance, size=N_importance//4, replace=True, p=weights_importance/np.sum(weights_importance))

# Metropolis-Hastings MCMC
alpha_prior = 1
beta_prior = 1
samples_mcmc = metropolis_hastings(log_posterior, theta_init, n_samples)

# Plot all distributions for comparison
plt.figure(figsize=(10, 6))
plt.plot(theta_values, posterior_dens, color='blue', lw=2, label='Analytical Posterior')
plt.hist(posterior_samples_importance, bins=60, density=True, color='purple', alpha=0.7, label='Importance Sampling')
plt.hist(samples_mcmc, bins=60, density=True, color='green', alpha=0.7, label='Posterior Distribution of θ (MCMC)')
plt.xlim([0.4, 0.9])
plt.ylim([0, 20])
plt.xlabel('θ')
plt.ylabel('Density')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

from scipy.stats import truncnorm, norm
import seaborn as sns

# Load the data from a URL
url = "https://raw.githubusercontent.com/yadavhimanshu059/CGS698C/main/notes/Data/word-recognition-times.csv"
dat = pd.read_csv(url).iloc[:, 1:]  # Read the data and exclude the first column

# Define the prior distribution for alpha (normally distributed)
def prior_alpha(alpha):
    # Returns the probability density of alpha with mean=400 and std=50
    return norm.pdf(alpha, 400, 50)

# Define the prior distribution for beta (truncated normal)
def prior_beta(beta):
    # Parameters for truncated normal distribution with bounds (0, ∞)
    a, b = (0 - 0) / 50, (np.inf - 0) / 50
    # Returns the probability density of beta within the bounds
    return truncnorm.pdf(beta, a, b, loc=0, scale=50)

# Define the likelihood function
def likelihood(alpha, beta, sigma, data):
    # Calculate mean (mu) based on the regression model
    mu = alpha + beta * data['type'].apply(lambda x: 1 if x == 'non-word' else 0)
    # Returns the product of normal densities (likelihood)
    return np.prod(norm.pdf(data['RT'], mu, sigma))

# Define the log-posterior function
def log_posterior(alpha, beta, sigma, data):
    # Compute log-prior using the defined prior functions
    log_prior = np.log(prior_alpha(alpha)) + np.log(prior_beta(beta))
    # Compute log-likelihood using the defined likelihood function
    log_likelihood = np.sum(norm.logpdf(data['RT'], alpha + beta * data['type'].apply(lambda x: 1 if x == 'non-word' else 0), sigma))
    # Return the sum of log-prior and log-likelihood
    return log_prior + log_likelihood

# Define the Metropolis-Hastings MCMC sampler
def metropolis_hastings(log_posterior, initial_values, iterations, sigma, data):
    # Initialize chains for alpha, beta, and sigma
    alpha_chain = [initial_values[0]]
    beta_chain = [initial_values[1]]
    sigma_chain = [initial_values[2]]

    for _ in range(iterations):
        # Get the last sampled values
        current_alpha = alpha_chain[-1]
        current_beta = beta_chain[-1]
        current_sigma = sigma_chain[-1]

        # Propose new values for alpha, beta, and sigma
        proposal_alpha = norm.rvs(current_alpha, 0.5)  # Propose alpha from normal distribution
        proposal_beta = truncnorm.rvs((0 - current_beta) / 50, (np.inf - current_beta) / 50, loc=current_beta, scale=0.5)  # Propose beta from truncated normal
        proposal_sigma = sigma  # Keep sigma fixed for simplicity

        # Calculate log-posterior for current and proposed values
        current_log_posterior = log_posterior(current_alpha, current_beta, current_sigma, data)
        proposal_log_posterior = log_posterior(proposal_alpha, proposal_beta, proposal_sigma, data)

        # Calculate acceptance ratio
        acceptance_ratio = np.exp(proposal_log_posterior - current_log_posterior)

        # Accept or reject the proposal
        if np.random.rand() < acceptance_ratio:
            alpha_chain.append(proposal_alpha)
            beta_chain.append(proposal_beta)
            sigma_chain.append(proposal_sigma)
        else:
            alpha_chain.append(current_alpha)
            beta_chain.append(current_beta)
            sigma_chain.append(current_sigma)

    return np.array(alpha_chain), np.array(beta_chain), np.array(sigma_chain)

# Initialize parameters for MCMC
initial_alpha = 400  # Initial value for alpha
initial_beta = 1     # Initial value for beta
sigma = 30           # Initial value for sigma
iterations = 5000    # Number of MCMC iterations

# Run the Metropolis-Hastings MCMC sampler
alpha_chain, beta_chain, sigma_chain = metropolis_hastings(log_posterior, [initial_alpha, initial_beta, sigma], iterations, sigma, dat)

# Discard the initial 'burn-in' samples to ensure convergence
burn_in = 1000
alpha_chain = alpha_chain[burn_in:]
beta_chain = beta_chain[burn_in:]
sigma_chain = sigma_chain[burn_in:]

# Calculate posterior means for alpha and beta
alpha_mean = np.mean(alpha_chain)
beta_mean = np.mean(beta_chain)
print(f'Posterior mean of Alpha: {alpha_mean}')
print(f'Posterior mean of Beta: {beta_mean}')

# Calculate 95% credible intervals for alpha and beta
alpha_ci = np.percentile(alpha_chain, [2.5, 97.5])
beta_ci = np.percentile(beta_chain, [2.5, 97.5])
print(f"95% Credible Interval for Alpha: {alpha_ci}")
print(f"95% Credible Interval for Beta: {beta_ci}")