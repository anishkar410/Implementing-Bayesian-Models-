# -*- coding: utf-8 -*-
"""A Gaussian model of reading_The Bayesian learning_Model building in the Bayesian framework

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WZqvcc8c7w8NPnxJ2sANU2Lj7tNHBvyH
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, truncnorm
import pandas as pd
import scipy.special as sp

# Part 1: Posterior density for different values of θ
y = 7  # Number of successes
n = 10  # Number of trials

# Likelihood function for a given θ
def likelihood(theta, y, n):
    comb = sp.comb(n, y)  # Binomial coefficient
    return comb * (theta ** y) * ((1 - theta) ** (n - y))

# Prior function for θ (uniform prior)
def prior(theta):
    return 1 if 0 <= theta <= 1 else 0

# Posterior density function (uniform prior scaled by 11)
def posterior_density(theta, y, n):
    L = likelihood(theta, y, n)
    return 11 * L * prior(theta)

# Compute and print posterior densities for specific values of θ
theta_values = [0.75, 0.25, 1]
for theta in theta_values:
    print(f"Posterior density for θ = {theta} is {posterior_density(theta, y, n)}")

# Plot posterior distribution
theta_vector = np.linspace(0, 1, 1000)
posterior_densities_vector = [posterior_density(theta, y, n) for theta in theta_vector]
max_index = np.argmax(posterior_densities_vector)  # Index of max posterior
max_theta = theta_vector[max_index]
max_posterior = posterior_densities_vector[max_index]

plt.figure(figsize=(10, 6))
plt.plot(theta_vector, posterior_densities_vector, label='Posterior Density $p(\\theta|y)$')
plt.axvline(x=max_theta, color='red', linestyle='--', label=f'Max at $\\theta={max_theta:.2f}$')
plt.xlabel('$\\theta$')
plt.ylabel('$p(\\theta|y)$')
plt.title('Posterior Distribution of $\\theta$')
plt.legend()
plt.grid(True)
plt.show()

# Part 2: Gaussian model of reading

# Given data
y = np.array([300, 270, 390, 450, 500, 290, 680, 450])
sigma = 50  # Standard deviation for both word and non-word recognition times
mu_0 = 250  # Mean of the prior distribution
sigma_0 = 25  # Standard deviation of the prior distribution

# Likelihood function for given μ and y
def likelihood(mu, y, sigma):
    sum_ex_power = np.sum((y - mu) ** 2)  # Sum of squared differences
    L = (1 / (np.sqrt(2 * np.pi * sigma ** 2) ** len(y))) * np.exp(-sum_ex_power / (2 * sigma ** 2))
    return L

# Prior function for μ (normal distribution)
def prior(mu, mu_0, sigma_0):
    return (1 / (np.sqrt(2 * np.pi * sigma_0 ** 2))) * np.exp(-((mu - mu_0) ** 2) / (2 * sigma_0 ** 2))

# Unnormalized posterior density function
def unnormalized_posterior(mu, y, sigma, mu_0, sigma_0):
    L = likelihood(mu, y, sigma)
    p = prior(mu, mu_0, sigma_0)
    return L * p

# Calculate and print unnormalized posterior densities for specific values of µ
mu_values = [300, 900, 50]
for mu in mu_values:
    print(f"Unnormalized posterior density for µ = {mu} is {unnormalized_posterior(mu, y, sigma, mu_0, sigma_0)}")

# Plot unnormalized posterior distribution
mu_vector = np.linspace(0, 1000, 2000)
unnormalized_posterior_vector = [unnormalized_posterior(mu, y, sigma, mu_0, sigma_0) for mu in mu_vector]
max_index = np.argmax(unnormalized_posterior_vector)  # Index of max posterior
max_mu = mu_vector[max_index]
max_posterior = unnormalized_posterior_vector[max_index]

plt.figure(figsize=(10, 6))
plt.plot(mu_vector, unnormalized_posterior_vector, label='Unnormalized Posterior')
plt.axvline(x=max_mu, color='red', linestyle='--', label=f'Max at $\\mu={max_mu:.2f}$')
plt.xlabel('$\\mu$')
plt.ylabel('Posterior')
plt.title('Unnormalized Posterior Distribution of $\\mu$')
plt.legend()
plt.grid(True)
plt.show()

# Part 3: Bayesian learning

# 3.1 Prior on λ for day 5
k = np.array([25, 20, 23, 27])  # Number of accidents observed each day
parameter1 = 40  # Initial shape parameter of the Gamma prior
parameter2 = 2   # Initial rate parameter of the Gamma prior
for i in k:
    parameter1 += i  # Update shape parameter
    parameter2 += 1  # Update rate parameter

print(f"After 4 days, Parameter 1 = {parameter1} and Parameter 2 = {parameter2}")
print(f"Prior on λ to generate predictions for day 5 is Gamma({parameter1}, {parameter2})")

# 3.2 Predicted number of road accidents on day 5
mean_lambda = parameter1 / parameter2  # Mean of the Gamma distribution
print(f"Predicted number of road accidents on day 5 is approximately {mean_lambda:.1f}")

# Part 4: Model Building in Bayesian Framework

# 4.5.1 Unnormalized posterior distribution of µ for Null hypothesis model
url = "https://raw.githubusercontent.com/yadavhimanshu059/CGS698C/main/notes/Module2/recognition.csv"
data = pd.read_csv('/content/recognition.csv')
Tw = data['Tw']  # Word recognition times
Tnw = data['Tnw']  # Non-word recognition times

mean = 300  # Prior mean for μ
sigma = 60  # Standard deviation for recognition times
delta = 0   # Initial difference between word and non-word recognition times

# Likelihood function for the Null hypothesis model
def likelihood(mu, delta):
    likelihood_tw = norm.pdf(Tw, loc=mu, scale=sigma).prod()  # Likelihood for word recognition times
    likelihood_tnw = norm.pdf(Tnw, loc=mu + delta, scale=sigma).prod()  # Likelihood for non-word recognition times
    return likelihood_tw * likelihood_tnw

# Prior functions for μ and δ
def prior_mu(mu):
    return norm.pdf(mu, loc=300, scale=50)  # Normal prior for μ

def prior_delta(delta):
    return truncnorm.pdf(delta, a=0, b=np.inf, loc=0, scale=50)  # Truncated normal prior for δ

# Calculate unnormalized posterior density for µ
mu_values = np.linspace(200, 400, 500)
delta_values = np.linspace(-100, 100, 500)
posterior_values = np.zeros(len(mu_values))

for i, mu in enumerate(mu_values):
    for delta in delta_values:
        posterior_values[i] += likelihood(mu, delta) * prior_mu(mu) * prior_delta(delta)
posterior_values /= posterior_values.sum()  # Normalize posterior

# Plot unnormalized posterior distribution of µ
plt.figure(figsize=(10, 6))
plt.plot(mu_values, posterior_values, label='Unnormalized Posterior', linestyle='-')
plt.xlabel('µ')
plt.ylabel('Density')
plt.title('Unnormalized Posterior Distribution of µ for Null Hypothesis Model')
plt.grid(True)
plt.legend()
plt.show()

# 4.5.2 Prior predictions from the lexical-access model
n = 1000  # Number of samples
mu_samples = np.random.normal(mean, 50, n)  # Sample values for μ
delta_samples = truncnorm.rvs(a=0, b=np.inf, loc=0, scale=50, size=n)  # Sample values for δ

# Generate word and non-word recognition times
lex_nw_time = mu_samples + delta_samples + np.random.normal(0, sigma, n)
lex_w_time = mu_samples + np.random.normal(0, sigma, n)

# Plot histograms for prior predictions
plt.figure(figsize=(7, 6))
plt.hist(lex_nw_time, bins=30, alpha=0.5, label='Non-Word Recognition Times', color='pink')
plt.hist(lex_w_time, bins=30, alpha=0.5, label='Word Recognition Times', color='green')
plt.xlabel('Recognition times')
plt.ylabel('Frequency')
plt.title('Lexical-Access Model Prior Prediction')
plt.legend()
plt.grid(True)
plt.show()

# 4.5.3 Compare prior predictions of Null hypothesis and Lexical access models
null_nw_time = np.random.normal(mean, sigma, n)  # Null hypothesis model non-word recognition times
null_w_time = np.random.normal(mean, sigma, n)  # Null hypothesis model word recognition times

# Plot histograms for comparison
plt.figure(figsize=(12, 10))
plt.subplot(2, 1, 1)
plt.hist(null_w_time, bins=30, alpha=0.5, label='Word', color='purple')
plt.hist(null_nw_time, bins=30, alpha=0.5, label='Non-Word', color='orange')
plt.xlabel('Recognition Times')
plt.ylabel('Frequency')
plt.title('Null Hypothesis Model')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.hist(lex_nw_time, bins=30, alpha=0.5, label='Non-Word Recognition Times', color='pink')
plt.hist(lex_w_time, bins=30, alpha=0.5, label='Word Recognition Times', color='green')
plt.xlabel('Recognition times')
plt.ylabel('Frequency')
plt.title('Lexical-Access Model Prior Prediction')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# 4.5.4 Compare model predictions with observed data
plt.figure(figsize=(12, 10))
plt.subplot(2, 2, 1)
plt.hist(null_w_time, bins=50, density=True, alpha=0.7, color='skyblue', label='Prior')
plt.hist(Tw, bins=20, density=True, alpha=0.3, color='gray', label='Observed Data')
plt.scatter(Tw, np.zeros_like(Tw), color='red', zorder=10)
plt.title('Null Model / Observed (Tw)')
plt.xlabel('Recognition Time')
plt.ylabel('Density')
plt.legend()

plt.subplot(2, 2, 2)
plt.hist(null_nw_time, bins=50, density=True, alpha=0.7, color='pink', label='Prior')
plt.hist(Tnw, bins=20, density=True, alpha=0.3, color='gray', label='Observed Data')
plt.scatter(Tnw, np.zeros_like(Tnw), color='red', zorder=10)
plt.title('Null Model / Observed (Tnw)')
plt.xlabel('Recognition Time')
plt.ylabel('Density')
plt.legend()

plt.subplot(2, 2, 3)
plt.hist(lex_w_time, bins=50, density=True, alpha=0.7, color='coral', label='Prior')
plt.hist(Tw, bins=15, density=True, alpha=0.3, color='gray', label='Observed Data')
plt.scatter(Tw, np.zeros_like(Tw), color='red', zorder=10)
plt.title('Lexical Model / Observed (Tw)')
plt.xlabel('Recognition Time')
plt.ylabel('Density')
plt.legend()

plt.subplot(2, 2, 4)
plt.hist(lex_nw_time, bins=50, density=True, alpha=0.7, color='lightgreen', label='Prior')
plt.hist(Tnw, bins=15, density=True, alpha=0.3, color='gray', label='Observed Data')
plt.scatter(Tnw, np.zeros_like(Tnw), color='red', zorder=10)
plt.title('Lexical Model / Observed (Tnw)')
plt.xlabel('Recognition Time')
plt.ylabel('Density')
plt.legend()

plt.tight_layout()
plt.show()

# 4.5.5 Unnormalized posterior distribution of δ for the lexical-access model
def unnormalized_posterior_lexical(delta, mu, Tw, Tnw, sigma):
    likelihood_tw = norm.pdf(Tw, loc=mu, scale=sigma).prod()
    likelihood_tnw = norm.pdf(Tnw, loc=mu + delta, scale=sigma).prod()
    return likelihood_tw * likelihood_tnw * prior_mu(mu) * prior_delta(delta)

delta_values = np.linspace(0, 200, 1000)
mu_value = mean
posterior_delta_values = [unnormalized_posterior_lexical(delta, mu_value, Tw, Tnw, sigma) for delta in delta_values]

# Plot unnormalized posterior distribution of δ
plt.plot(delta_values, posterior_delta_values)
plt.xlabel('δ')
plt.ylabel('Unnormalized Posterior')
plt.title('Unnormalized Posterior Distribution of δ (Lexical-Access Model)')
plt.grid(True)
plt.show()